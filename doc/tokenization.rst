Tokenization
************

============
Tokenization
============
Tokenization is a language-dependant task that splits a text into a list of tokens.


===================
Treebank tokenizers
===================

.. autoclass:: tagenwa.tokenize.treebank.GenericTreebankWordTokenizer
	:members:
	:undoc-members:

.. autoclass:: tagenwa.tokenize.treebank.EnglishTreebankWordTokenizer
	:members:
	:undoc-members:
